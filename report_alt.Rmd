---
title: "The Journey from Wild to Text Book Data: A Case Study of the National Longitudinal Survey of Youth"
authors:
  - name: Dewi Amaliah
    thanks: ""
    department: Department of Econometrics and Business Statistics
    affiliation: Monash University
    location: Clayton, VIC 3800
    email: ""
  - name: Kate Hyde
    thanks: ""
    department: ""
    affiliation: ""
    location: ""
    email: ""
  - name: Emi Tanaka
    thanks: emitanaka.org
    department: Department of Econometrics and Business Statistics
    affiliation: Monash University
    location: Clayton, VIC 3800
    email: emi.tanaka@monash.edu
  - name: Nicholas Tierney
    thanks: ""
    department: ""
    affiliation: ""
    location: ""
    email: ""
  - name: Dianne Cook
    thanks: dicook.org
    department: Department of Econometrics and Business Statistics
    affiliation: Monash University
    location: Clayton, VIC 3800
    email: dicook@monash.edu
bibliography: references.bib
biblio-style: unsrt
#preamble: >
output: 
  rticles::arxiv_article:
    keep_tex: false
date: "24/02/2021"
always_allow_html: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE,
                      cache = TRUE, 
                      cache.path = "cache/",
                      fig.path = "figures/")
library(tidyverse)
library(brolgar)
library(patchwork)
library(kableExtra)
library(MASS)
library(janitor)
library(DiagrammeR)
library(rsvg)
library(webshot)
library(mgcv)
library(modelr)
library(tsibble)
```


The NSLY79 is a prominent open data source that has been playing an important role in multidisciplinary research. This article aims to demonstrate the steps of initial data analysis (IDA), namely tidying, cleaning, and documenting the process to make the data available for text book or research data. Besides, this article provides the opportunity to continuously refresh the text book data by updating the codes that are provided in this article. The IDA shows that the NLSY79 data has two problems. Firstly, the downloaded data format is untidy. Secondly, some anomalies in terms of extreme high hourly wages are found. We addressed the first problem by tidying the data according to three principles of a tidy data. For the latter problem, we found the anomalies using the robustness weight threshold of robust linear regression. An observation with weight < 1.12 is considered as an outlier. Further, we fixed the anomalies using the predicted value of the robust linear regression model. Using the cleaned data, we provided three data sets, the high school mean hourly wage data, the high school dropouts mean hourly wage data, and demographic variables of the NLSY79 cohort. These data sets are contained in an open source R package `yowie`. Supplementary materials for this article are available online. 

**Keywords**: Data cleaning; Data tidying; Longitudinal data; NLSY79; Open data; Initial data analysis; Outlier detection; Robust linear regression





# Introduction


"Open data" is data that are freely accessible, modifiable, and shareable by anyone for any purpose [@opendata]. This type of data can be useful as example data in statistical text books and for research purposes. However, open data are often, what we might call "wild data", because it requires substantial cleaning and tidying to tame it into text book shape. @HuebnerMariannePhD2016Asat emphasize that making the data cleaning process accountable and transparent is imperative. Documenting the data cleaning is essential for the integrity of downstream statistical analyses and model building [@HuebnerMarianne2020Haar]. 

Data cleaning is part of what is called "initial data analysis (IDA)" [@Chatfield1985TIEo]. The other IDA steps are to explore the data properties and check assumption of modeling to document and report the process for the later formal analysis. Hence, exploratory data analysis, which is defined by @tukey as a detective work to get the clue from data, either numerically or graphically, before confirmatory data analysis is performed, encompasses IDA. @DasuTamraparni2003Edma say that data cleaning and exploration, without naming it as IDA, is a difficult task and consumes 80% of the data mining task. 

Despite its importance, this IDA stage is often undervalued and neglected [@Chatfield1985TIEo]. There are few research papers that document the data cleaning [@WickhamHadley2014TD]. Furthermore, the decisions made in this stage often go unreported  in the sense that IDA is often performed in an unplanned and unstructured way, and is only shared among restricted parties [@HuebnerMarianne2020Haar]. 

This paper aims to demonstrate the steps of IDA,  tidying, cleaning, and documenting the process, for a prominent open data source, National Longitudinal Survey of Youth (NLSY) 1979, henceforth referred to NSLY79. This data has been playing an important role in research in various disciplines including but are not limited to economics, sociology, education, public policy, and public health for more than a quarter of the century [@MichaelRPergamit2001DWTN]. In addition, The National Longitudinal Survey is considered as survey with high retention rates and carefully designed making it suitable for a life course research [@MichaelRPergamit2001DWTN] and [@eliznlsy]. According to @eliznlsy, thousand of articles, and hundreds of book chapters and monographs has incorporated the NLSY data. Moreover, the NLSY79 is considered as the most widely used and most important cohort in the NLSY79 data sets [@MichaelRPergamit2001DWTN]. 

@SingerJudithD2003Alda used the  wages and other variables of high school dropouts from the NLSY79 data as an example data set to illustrate longitudinal data modeling. Our aim is to refresh this text book data to be more up to date. Here, we investigate the process of getting from the raw NLSY79 to this text book data set. However, we are not able to create the exact same data set as published in their book since we do not have the information of what age threshold did they use to determine the high school dropouts.

This paper is structured to have 5 sections. Section 2 describes the original data source. Section 3 presents the steps of cleaning the data, including getting and tidying the data from the NLSY79 and initial data analysis to find and treat the anomalies in the data. We also gave examples of exploratory data analysis using the clean data in Section 4. Finally, Section 6 summarizes the paper. 

# The NLSY79

The NLSY79 is a longitudinal survey held by the U.S Bureau of Labor Statistics that follows the lives of a sample of American youth and born between 1957-1964 [@nlsy79]. The cohort originally included 12,686 respondents ages 14-22 when first interviewed in 1979. It was comprised of Blacks, Hispanics, economically disadvantaged non-Black non-Hispanics, and youth in the military. In 1984 and 1990, two sub-samples were dropped from the interview; the dropped subjects are the 1,079 members of the military sample and 1,643 members of the economically disadvantaged non-Black non-Hispanics respectively. Hence, 9,964 respondents remain in the eligible samples. The surveys were conducted annually from 1979 to 1994 and biennially thereafter. Data are now available from Round 1 (1979 survey year) to Round 28 (2018 survey year).

Although the main focus area of the NLSY is labor and employment, the NLSY also cover several other topics including education; training and achievement; household, geography and contextual variables; dating, marriage, cohabitation; sexual activity, pregnancy, and fertility; children; income, assets and program participation; health; attitudes and expectations; and crime and substance use. 

There are two ways to conduct the interview of the NLSY79, which are face to face interview or by telephone. In recent survey years, more than 90 percent of respondents were interviewed by telephone [@eliznlsy]. 

# The NLSY79 Data Cleaning 

## Getting and Tidying the Data

The NLSY79 data are stored in a [database](https://www.nlsinfo.org/content/cohorts/nlsy79/get-data) and could be downloaded by variables. Several variables are available for download and could be browsed by index. For the wages data set, we only extracted these variables:

- Education, Training & Achievement Scores
    -  Education -> Summary measures -> All schools -> By year -> Highest grade completed
       - Downloaded all of the 78 variables in Highest grade completed.
- Employment
    - Summary measures -> By job -> Hours worked and Hourly wages
      - Downloaded all of the 427 variables in Hours worked
      - Downloaded all of the 151 variables in Hourly wages
      
      Both hours worked and hourly wages are recorded by the job, up to five jobs for each id/subject. 
- Household, Geography & Contextual Variables
    - Context -> Summary measures -> Basic demographics 
      - Downloaded year and month of birth, race, and sex variables.
      
      There are two versions of the year and month of birth, i.e. 1979 and 1981 data. We downloaded these two versions.
      
The downloaded data set came in a csv (NLSY79.csv) and dat (NLSY79.dat) format. We only used the .dat format. It also came along with these files:

- NLSY79.NLSY79: This is the tagset of variables that can be uploaded to the web site to recreate the data set.
- NLSY79.R: This is an R script provided automatically by the database for reading the data into R and convert the variables' name and its label into something more sensible. We utilized this code to do an initial data tidying. It produced two data set,  `categories_qnames` (the observations are stored in categorical/interval values) and `new_data_qnames` (the observations are stored in integer form).

```{r, cache = TRUE}
source(here::here("data-raw/NLSY79/NLSY79.R"))
```

According to @WickhamHadley2014TD, a tidy data sets comply with three rules, the first is that each variable forms a column, the second is that each observation forms a row, and the last is that each type of observational unit forms a table. Unfortunately, the `new_data_qnames` did not meet these requirements in the way that the value for particular year and job are stored in different columns, hence the data contains a huge number of columns (686 columns). The example of the untidy of the data set is displayed in Table \ref{tab:untidy-data}, which actually intended to display the hourly rate of each respondent by job (HRP1 to HRP5) and by year (1979 and 1980). The table implies that the column headers are values of the year and job, not variable names. 

```{r untidy-data, echo = FALSE}

untidy_demo <- new_data_qnames %>%
  as_tibble() %>%
  # in 2018, the variable's name is Q3-4_2018, instead of HGC_2018
  rename(HGC_2018 = `Q3-4_2018`) %>%
  dplyr::select(CASEID_1979,
            starts_with("HRP") &
              ends_with(c("1979", "1980", "1981", "1982", "1983"))) %>%
  dplyr::select(1:7) %>%
  head()

kable(untidy_demo,
      caption = "The Untidy Form of the NLSY79 Raw Data") %>%
  kable_styling(latex_options = "striped")
```

Consequently, the data should be tidied and wrangled first to extract the demographic and employment variables that we want to put in the final data set. We mainly used `tidyr` [@tidyr], `dplyr` [@dplyr], and `stringr` [@stringr] to do this job.

### Tidying demographic variables

`Age in 1979`, `gender`, `race`, `highest grade completed` (factor and integer), and the `year when the highest grade completed` are the variables that we want to use in the cleaned data set. 

Age in 1979 are derived from year of birth and month of birth variables in the raw data set. The variables have two versions, which are the 1979 version and the 1981 version. We only used the 1979 data and did a consistency check of those years and flag the inconsistent observations. 

```{r dob-tidy}
## tidy the date of birth data
dob_tidy <- new_data_qnames %>%
  dplyr::select(CASEID_1979,
         starts_with("Q1-3_A~")) %>%
  mutate(dob_year = case_when(
                    # if the years recorded in both sets match, take 79 data
                    `Q1-3_A~Y_1979` == `Q1-3_A~Y_1981` ~ `Q1-3_A~Y_1979`,
                    # if the year in the 81 set is missing, take the 79 data
                    is.na(`Q1-3_A~Y_1981`) ~ `Q1-3_A~Y_1979`,
                    # if the sets don't match for dob year, take the 79 data
                    `Q1-3_A~Y_1979` != `Q1-3_A~Y_1981` ~ `Q1-3_A~Y_1979`),
        dob_month = case_when(
                    # if months recorded in both sets match, take 79 data
                    `Q1-3_A~M_1979` == `Q1-3_A~M_1981` ~ `Q1-3_A~M_1979`,
                    # if month in 81 set is missing, take the 79 data
                    is.na(`Q1-3_A~M_1981`) ~ `Q1-3_A~M_1979`,
                    # if sets don't match for dob month, take the 79 data
                    `Q1-3_A~M_1979` != `Q1-3_A~M_1981` ~ `Q1-3_A~M_1979`),
        # flag if there is a conflict between dob recorded in 79 and 81
        dob_conflict = case_when(     
                      (`Q1-3_A~M_1979` != `Q1-3_A~M_1981`) & !is.na(`Q1-3_A~M_1981`)
                      ~ TRUE,
                      (`Q1-3_A~Y_1979` != `Q1-3_A~Y_1981`) & !is.na(`Q1-3_A~Y_1981`)
                      ~ TRUE,
                      (`Q1-3_A~Y_1979` == `Q1-3_A~Y_1981`) & 
                      (`Q1-3_A~M_1979` == `Q1-3_A~M_1981`) ~ FALSE,
                      is.na(`Q1-3_A~M_1981`) | is.na(`Q1-3_A~Y_1981`) ~ FALSE)) %>%
  dplyr::select(CASEID_1979,
         dob_month,
         dob_year,
         dob_conflict)
```


For `gender` and `race`, we only renamed these variables. 

```{r demog-tidy}
## tidy the gender and race variables
demog_tidy <- categories_qnames %>%
  dplyr::select(CASEID_1979,
         SAMPLE_RACE_78SCRN,
         SAMPLE_SEX_1979) %>%
  rename(gender = SAMPLE_SEX_1979,
         race = SAMPLE_RACE_78SCRN)
```

The `highest grade completed` came with several version in each year. We chose the revised May data because the May data seemed to have less missing and presumably the revised data has been checked. However, there was no revised May data for 2012, 2014, 2016, and 2018 so we just used the ordinary May data. 

```{r tidy-grade}
# tidy the grade 
demog_education <- new_data_qnames %>%
  as_tibble() %>%
  # in 2018, the variable's name is Q3-4_2018, instead of HGC_2018
  rename(HGC_2018 = `Q3-4_2018`) %>%
  dplyr::select(CASEID_1979,
         starts_with("HGCREV"),
         "HGC_2012",
         "HGC_2014",
         "HGC_2016",
         "HGC_2018") %>%
  pivot_longer(!CASEID_1979,
               names_to = "var",
               values_to = "grade") %>%
  separate("var", c("var", "year"), sep = -4) %>%
  filter(!is.na(grade)) %>%
  dplyr::select(-var)
```

In the final data, we only used the highest grade completed ever and derived the year of when the highest grade completed its categorical value. Therefore, we wrangled the highest grade completed in each year to mutate these variables. 

```{r tidy-hgc}

## getting the highest year of completed education ever
highest_year <- demog_education %>%
  group_by(CASEID_1979) %>%
  mutate(hgc_i = max(grade)) %>%
  filter(hgc_i == grade) %>%
  filter(year == first(year)) %>%
  rename(yr_hgc = year) %>%
  dplyr::select(CASEID_1979, yr_hgc, hgc_i) %>%
  ungroup() %>%
  mutate('hgc' = ifelse(hgc_i == 0, "NONE", ifelse(hgc_i == 1, "1ST GRADE",
                 ifelse(hgc_i == 2, "2ND GRADE", ifelse(hgc_i == 3, "3RD GRADE",
                 ifelse(hgc_i >= 4 & hgc_i <= 12, paste0(hgc_i,"TH GRADE"),
                 ifelse(hgc_i == 13, "1ST YEAR COL",
                 ifelse(hgc_i == 14, "2ND YEAR COL",
                 ifelse(hgc_i == 15, "3RD YEAR COL",
                 ifelse(hgc_i == 95, "UNGRADED", 
                        paste0((hgc_i - 12), "TH YEAR COL")))))))))))
```

Finally, we join all the tidy variables in a data set called `full_demographics`.

```{r full-demog}
full_demographics <- full_join(dob_tidy, demog_tidy, by = "CASEID_1979") %>%
  full_join(highest_year, by = "CASEID_1979") %>%
  rename("id" = "CASEID_1979")

head(full_demographics)
```

### Tidying employment variables

The employment data comprises of three variables, i.e. `total hours of work per week`, `number of jobs that an individual has`, and `mean hourly wage`. For hours worked per week, initially only one version per job, no choice from 1979 to 1987 (QES-52A). From 1988 onward, when we had more options, we chose the variable for total hours including time spent working from home (QES-52D). However, 1993 did not have all the five D variables (the first one and the last one were missing), so we used QES-52A variable instead. In addition, 2008 only had jobs 1-4 for the QES-52D variable (whereas the other years had 1-5), so we just used these.

```{r tidy-hours}
# make a list for years where we used the "QES-52A"
year_A <- c(1979:1987, 1993)
#function to get the hour of work
get_hour <- function(year){
  if(year %in% year_A){
   temp <- new_data_qnames %>%
    dplyr::select(CASEID_1979,
            starts_with("QES-52A") &
              ends_with(as.character(year)))} 
  else{
    temp <- new_data_qnames %>%
    dplyr::select(CASEID_1979,
            starts_with("QES-52D") &
              ends_with(as.character(year)))} 
    temp %>% 
      pivot_longer(!CASEID_1979,
                 names_to = "job",
                 values_to = "hours_work") %>%
      separate("job", c("job", "year"), sep = -4) %>%
      mutate(job = paste0("job_", substr(job, 9, 10))) %>%
      rename(id = CASEID_1979)
}

# list to save the iteration result
hours <- list()
# getting the hours of work of all observations
for(ayear in c(1979:1994, 1996, 1998, 2000, 2002, 2004, 2006, 2008, 2010, 
               2012, 2014, 2016, 2018)) {
   hours[[ayear]] <- get_hour(ayear)
}
# unlist the hours of work
hours_all <- bind_rows(!!!hours)
```

The same algorithm was also deployed to tidy the rate of wage by year and by ID. The difference is that the hourly rate had only one version of each year. The hours of work and the hourly rate were then joined to calculate the number of jobs that an ID has and their mean hourly wage. Some observations had 0 in their hourly rate, which is considered as invalid value. Thus, their hourly rate set to be N.A.

```{r tidy-rate}
get_rate <- function(year) {
  new_data_qnames %>%
    dplyr::select(CASEID_1979,
            starts_with("HRP") &
              ends_with(as.character(year))) %>%
    pivot_longer(!CASEID_1979, names_to = "job", values_to = "rate_per_hour") %>%
    separate("job", c("job", "year"), sep = -4) %>%
    mutate(job = paste0("job_0", substr(job, 4, 4))) %>%
    rename(id = CASEID_1979)
}
rates <- list()
for(ayear in c(1979:1994, 1996, 1998, 2000, 2002, 2004, 2006, 2008, 2010, 
               2012, 2014, 2016, 2018)) {
   rates[[ayear]] <- get_rate(ayear)
}
rates_all <- bind_rows(!!!rates)
# join hours and rates variable
hours_wages <- left_join(rates_all, 
                         hours_all, 
                         by = c("id", "year", "job")) %>%
  # set the 0 value in rate_per_hour as NA 
  mutate(rate_per_hour = ifelse(rate_per_hour == 0, NA,
                                rate_per_hour))
head(hours_wages)
```

Since our ultimate goal is to calculate the mean hourly wage, the number of jobs is calculate based on the availability of the `rate_per_hour` information. For example, the number of jobs of ID 1, based on `hours_work`, is two. However, since the information of hourly rate of `job_02` is not available, the number of job is considered as 1. 

Further, we calculated the mean hourly wage for each ID in each year using a weighted mean with the hours of work as the weight. However, there are a lot of missing value in `hours_work` variable. In that case, we only calculated the mean hourly wage based on arithmetic/regular mean method. Hence, we created a new variable to flag whether the mean hourly wage is a weighted or a regular mean. Additionally, if an ID only had one job, we directly used their hourly wages information and flagged it as an arithmetic mean.

```{r}
# calculate number of jobs that a person has in one year
no_job <- hours_wages %>%
  filter(!is.na(rate_per_hour)) %>%
  group_by(id, year) %>%
  summarise(no_jobs = length(rate_per_hour))

# filter the observations with available rate per hour
eligible_wages <- hours_wages %>%
  filter(!is.na(rate_per_hour)) %>%
  left_join(no_job, by = c("id", "year")) 

# calculate the mean_hourly_wage
# flag1 = code 1 for weighted mean
# code 0 for arithmetic mean
mean_hourly_wage <- 
  eligible_wages %>%
  group_by(id, year) %>%
  #calculate the weighted mean if the number of jobs > 1
  mutate(wages = ifelse(no_jobs == 1, rate_per_hour/100,
                        weighted.mean(rate_per_hour, hours_work, na.rm = TRUE)/100)) %>%
  #give the flag if it the weighted mean
  mutate(flag1 = ifelse(!is.na(wages) & no_jobs != 1, 1,
                        0)) %>%
  #calculate the arithmetic mean for the na
  mutate(wages = ifelse(is.na(wages), mean(rate_per_hour)/100,
                        wages)) %>%
  group_by(id, year) %>%
  summarise(wages = mean(wages),
            total_hours = sum(hours_work),
            number_of_jobs = mean(no_jobs),
            flag1 = mean(flag1)) %>%
  mutate(year = as.numeric(year)) %>%
  ungroup() %>%
  rename(mean_hourly_wage = wages) %>%
  mutate(is_wm = ifelse(flag1 == 1, TRUE,
                        FALSE)) %>%
  dplyr::select(-flag1)

head(mean_hourly_wage, n = 10)
```

The `mean_hourly_wage` and `full_demographic` data are then joined. We also filtered the data to only have the cohort who completed the education up to 12th grade and participated at least five rounds in the survey and save it to an object called `wages_demog_hs`. 

```{r}
# join the wages information and the demographic information by case id.
wages_demog <- left_join(mean_hourly_wage, full_demographics, by="id")
# calculate the years in work force and the age of the subjects in 1979
wages_demog <- wages_demog %>%
  mutate(yr_hgc = as.numeric(yr_hgc)) %>%
  mutate(years_in_workforce = year - yr_hgc) %>%
  mutate(age_1979 = 1979 - (dob_year + 1900))
# filter only the id with high school education
wages_demog_hs <- wages_demog  %>% filter(grepl("GRADE", hgc))
# calculate the number of observation
keep_me <- wages_demog_hs %>%
  count(id) %>%
  filter(n > 4)
wages_demog_hs <- wages_demog_hs %>%
  filter(id %in% keep_me$id)
```

## Initial Data Analysis

According to @HuebnerMariannePhD2016Asat, Initial Data Analysis (IDA) is the step of inspecting and screening the data after being collected to ensure that the data is clean, valid, and ready to be deployed in the later formal statistical analysis. Moreover, @Chatfield1985TIEo argued that the two main objectives of IDA is data description, which is to assess the structure and the quality of the data; and model formulation without any formal statistical inference. 

In this paper, we conducted an IDA or a preliminary data analysis to assess the consistency of the data with the cohort information that is provided by the NLSY. In addition, we also aimed to find the anomaly in the wages values using this approach. We mainly used graphical summary to do the IDA using `ggplot2`[@ggplot2] and `brolgar` [@brolgar]. 

As stated previously, the respondents' ages ranged from 12 to 22 when first interviewed in 1979. Hence, we would like to validate whether all of the respondents were in this range. Additionally, the [NLSY](https://www.nlsinfo.org/content/cohorts/nlsy79/intro-to-the-sample/nlsy79-sample-introduction) also provided the number of the survey cohort by their gender (6,403 males and 6,283 females) and race (7,510 Non-Black/Non-Hispanic; 3,174 Black; 2,002 Hispanic). To validate this, we used the `full_demographic` i.e. the data with the survey years 1979 sample. Table \ref{tab:age-table} and Table \ref{tab:gender-race-table} suggest that the demographic data we had is consistent with the sample information in the database.

```{r age-table, echo = FALSE}
age_table <- yowie::demographic_nlsy79 %>%
  group_by(age_1979) %>%
  count(age_1979) 

kable(age_table,
      caption = "Age Distribution of the NLSY79 samples",
      col.names = c("Age", "Number of Sample")) %>%
  kable_styling(latex_options = "striped")
```


```{r gender-race-table, echo = FALSE}
gender_race_table <- yowie::demographic_nlsy79 %>%
  tabyl(gender, race) %>%
  adorn_totals(c("row", "col")) %>%
  adorn_percentages("row") %>%
  adorn_pct_formatting(digits = 2) %>%
  adorn_ns(position = "front") %>%
  mutate(gender = ifelse(gender == "MALE", "Male",
                         ifelse(gender == "FEMALE", "Female", "Total")))

kable(gender_race_table,
      caption = "Gender and Race Distribution of the NLSY79 Samples",
      col.names = c("Gender", "Hispanic", "Black", "Non-Black, Non-Hispanic", "Total")) %>%
  kable_styling(latex_options = "striped") %>%
  add_header_above(c(" " = 1, "Race" = 3, " " = 1))

```

```{r featureplot, echo = FALSE, fig.cap = "Two plots showing the distribution of the mean hourly wage. Plot A portrays the pattern of mean hourly wage of high school cohort from 1979 to 2018 of each ID in US Dollar; Plot B shows the distribution of their minimum, median, and maximum value. We can see that some IDs had an extremely high of wages and it made the distribution of the three features is extremely skewed.", fig.width=6, fig.height=4}

spag <- wages_demog_hs %>%
  ggplot(aes(x = year,
             y = mean_hourly_wage,
             group = id)) +
  geom_line(alpha = 0.1) +
  ggtitle("A)") +
  theme(plot.title = element_text(size = 10))


wages_demog_hs_tsibble <- as_tsibble(x = wages_demog_hs,
                    key = id,
                    index = year,
                    regular = FALSE)
wages_three_feat <- wages_demog_hs_tsibble %>%
  features(mean_hourly_wage, 
           feat_three_num
           )
wages_feat_long <- wages_three_feat %>%
  pivot_longer(c(min, med, max), names_to = "feature", values_to = "value")
feature <- ggplot(wages_feat_long) +
  geom_density(aes(x = value, colour = feature, fill = feature), alpha = 0.3) +
  ggtitle("B)") +
  theme(plot.title = element_text(size = 10)) 
 
spag + feature
```


The next step is that we explored the mean hourly wage data, in this case, we only explored the wages data in `wages_demog_hs`. Figure \ref{fig:featureplot} conveys that there is clearly a problem in the mean hourly wage values. Figure \ref{fig:featureplot} A shows that some observations had an exceptionally high figure of wages, even more than US$10,000 per hour. In Figure \ref{fig:featureplot} B, we barely see any difference in the minimum, median, an maximum value of the wages since the distribution is heavily skewed to the right. Additionally, Table \ref{tab:summarytable} shows that the overall wages median of the cohort is only 7.2, while the mean is 11.87. It indicates that the data might contains a lot of extreme values. 



```{r summarytable, echo = FALSE}
kable(as.array(summary(wages_demog_hs$mean_hourly_wage)), 
      caption = "Summary Statistics of Wages of High School Data",
      col.names = c("Statistics", "Value")) %>%
  kable_styling(latex_options = "striped")
```

In Figure \ref{fig:high-wages}, we plotted some respondents with a high value of mean hourly wages. We filtered all of the IDs who earned more than US$ 500 per hour averagely. We found that most of these respondents only experienced one point of extremely high wages. Thus, we suspected that these high values are erroneous values resulted from a data entry error. 

```{r high-wages, echo = FALSE, fig.cap= "Some observations with extremely high mean hourly wage. Most of the IDs only have one point of high wage.", fig.height=8, fig.width=6}

wages_high <- filter(wages_demog_hs, mean_hourly_wage > 500) %>%
  as_tibble() 

wages_high2 <- wages_demog_hs %>%
  filter(id %in% wages_high$id)

ggplot(wages_high2) +
  geom_line(aes(x = year,
                y = mean_hourly_wage)) +
  geom_point(aes(x = year,
                y = mean_hourly_wage),
             size = 0.5,
             alpha = 0.5) +
  facet_wrap(~id, scales = "free_y", ncol = 5) +
  theme(axis.text.x = element_text(angle = 10, size = 6)) +
  ylab("mean hourly wage") 
  
```



Further, we took 36 samples randomly from the data and plotted it as is seen in Figure \ref{fig:sampleplot}. It implies that not only that some observations earned an extremely high figures of wages, but some also had a reasonably fluctuate wages, for example the IDs in panel number 5, 7, and 11. The plot also implies that the samples had a different pattern of mean hourly wages. Some had a flat wages for years but had a sudden increase in a year than it went down again, while the other experienced a upsurge in their wage, for instance the IDs in panel 9. 

According to @MichaelRPergamit2001DWTN, one of the flaws of the NLSY79 employment data is that since the NLSY79 collect the information of the working hours since the last interview, it might be challenging for the respondents to track the within-job hours changes that happens between survey year, especially for the respondents with fluctuate working hours or whose job is seasonal. It even has been more challenging since 1994, where the respondents had to recall two years period. This shortcoming might also contribute on the fluctuation of one's wages data. 

```{r sampleplot, echo = FALSE, fig.cap="The mean hourly wages of some random samples are shown in twelve facets, three IDs per facet. It suggests that some IDs had a reasonably fluctuate wages.", fig.height=4, fig.width=5}

set.seed(20210225)

ggplot(wages_demog_hs_tsibble, 
       aes(x = year,
                y = mean_hourly_wage,
                group = id)) +
  geom_line(alpha = 0.7) +
  facet_sample() +
  theme(axis.text.x = element_text(angle = 10, size = 6)) +
  ylab("mean hourly wage") 
```

### Robust Linear Model for Noises Treatment

As it is seen from figure \ref{fig:sampleplot}, there are many spikes in the mean hourly wage data. As part of the IDA, which is the model formulation, we built a robust linear regression model to address this issue. The notion of robust linear regression is to yield an estimation that is robust to the influence of noise or contamination [@KollerManuel2016rARP]. It also aims to detect the contamination by weighting each observation based on how "well-behave" they are, known as robustness weight. Observations with lower robustness weight are suggested as an outliers by this method [@KollerManuel2016rARP]. 

In this paper, we built the model using the `rlm` function from `MASS` package [@mass]. We set the `mean_hourly_wage` and `year` as the dependent and predictor respectively. Furthermore, we used M-Estimation with Huber weighting where the observation with small residual get a weight of 1, while the larger the residual, the smaller the weight (less than 1) [@rlm]. 

Since we worked with longitudinal data, we should built the model for each ID, instead of the overall data. The robust mixed model is actually the best model to be employed in this case. However, this method is too computationally and memory expensive, especially for a large data set, like the NLSY79 data. Thus, the model for each ID is built utilizing the `nest` and `map` function from `tidyr` [@tidyr] and `purrr` [@purrr] respectively. 

The challenging part of detecting the anomaly using the robustness weight is to determine the threshold of the weight where the observations considered as outliers. Moreover, it should be noted that not all the outliers is due to an error, instead it might be that one had a reasonably increasing or decreasing wages. To, minimize the risk of being mistakenly regard an outlier as an "erronous outlier", we have simulated some threshold and study the behavior of the spikes in each threshold. We found that 0.12 is the most reasonable value to be the threshold to minimize the risk of that drawback because it still capture the sensible spikes in the data. In other words, we kept maintaining the natural variability of the wages while minimizing the presence of anomalies because of the error in the data recording. After deciding the threshold, we imputed the observations with weight less than 0.2 wage with the models' predicted value. We then flagged those observations in a new variable called `is_pred`, 


```{r rlm, cache=TRUE, warning = FALSE, message = FALSE}
 
# nest the data by id to build a robust linear model
by_id <- wages_demog_hs %>%
  dplyr::select(id, year, mean_hourly_wage) %>%
  group_by(id) %>%
  nest()

# build a robust linear model
id_rlm <- by_id %>%
  mutate(model = map(.x = data,
                     .f = function(x){
                       rlm(mean_hourly_wage ~ year, data = x)
                     }))
# extract the property of the regression model
id_aug <- id_rlm %>%
  mutate(augmented = map(model, broom::augment)) %>%
  unnest(augmented)

# extract the weight of each observation
id_w <- id_rlm %>%
  mutate(w = map(.x = model,
                 .f = function(x){
                   x$w
                 })) %>%
  unnest(w) %>%
  dplyr::select(w)

# bind the property of each observation with their weight
id_aug_w <- cbind(id_aug, id_w) %>%
  dplyr::select(`id...1`,
                year,
                mean_hourly_wage,
                .fitted,
                .resid,
                .hat,
                .sigma,
                w) %>%
  rename(id = `id...1`)

# if the weight < 1, the mean_hourly_wage is replaced by the model's fitted/predicted value.
# and add the flag whether the observation is predicted value or not.
# since the fitted value is sometimes <0, and wages value could never be negative,
# we keep the mean hourly wage value even its weight < 1.

wages_rlm_dat <- id_aug_w %>%
  mutate(wages_rlm = ifelse(w < 0.12  & .fitted >= 0, .fitted,
                            mean_hourly_wage)) %>%
  mutate(is_pred = ifelse(w < 0.12 & .fitted >= 0, TRUE,
                          FALSE)) %>%
  dplyr::select(id, year, wages_rlm, is_pred)

# join back the `wages_rlm_dat` to `wages_demog_hs`

wages_demog_hs <- left_join(wages_demog_hs, wages_rlm_dat, by = c("id", "year"))

```

Figure \ref{fig:comppict} A shows that after imputing the anomalies with the models' predicted value, the highest wages value has decreased to be around $350. The spikes were still observed, but are not as extreme as the original data set. In Figure \ref{fig:comppict} B), we plotted the three features of mean hourly wages, namely the minimum, median, an maximum value, transformed to log scale. The plot implies that the skewness are slightly negative. We also see that the three features are overlapped each other. It indicates that some ID's minimum wage are higher than some ID's maximum wage. 


Furthermore, Figure \ref{fig:compare} implies that after the treatment, the fluctuation can still be observed in the data and only the large spikes, which are considered as "erronous outliers", are eliminated from the data. Hence, the model results a data set with the reasonable degree of fluctuation.

```{r comppict, echo = FALSE, fig.cap = "The distribution of the mean hourly wage after treating the extreme values. Plot A portrays the pattern of mean hourly wage of high school cohort from 1979 to 2018 of each ID in US Dollar; Plot B shows the distribution of their minimum, median, and maximum value transformed to log10 scale. We can see that some observations still had reasonbaly higher wages than the others. Also, some IDs' have a minimum wages that is higher than others' maximum wages.", fig.width=6, fig.height=4}

spag2 <- wages_demog_hs %>%
  ggplot(aes(x = year,
             y = wages_rlm,
             group = id)) +
  geom_line(alpha = 0.1) +
  ggtitle("A)") +
  theme(plot.title = element_text(size = 10))

wages_hs2020_rlm <- as_tsibble(x = wages_demog_hs,
                    key = id,
                    index = year,
                    regular = FALSE)
wages_three_feat_rlm <- wages_hs2020_rlm %>%
  features(wages_rlm, 
           feat_three_num
           )
wages_feat_long_rlm <- wages_three_feat_rlm %>%
  pivot_longer(c(min, med, max), names_to = "feature", values_to = "value")

feature2 <- ggplot(wages_feat_long_rlm) +
  geom_density(aes(x = value, colour = feature, fill = feature), alpha = 0.3) +
  ggtitle("B)") +
  scale_x_log10() + 
  theme(plot.title = element_text(size = 10)) +
  xlab("value (log10)")
 
spag2 + feature2
```

```{r compare-data, echo = FALSE}
set.seed(31251587)

sample_id <- sample(unique(wages_demog_hs$id), 20)
sample <- subset(wages_demog_hs, id %in% sample_id)

wages_compare <- sample %>%
  dplyr::select(id, year, mean_hourly_wage, wages_rlm) %>%
  rename(mean_hourly_wage_rlm = wages_rlm) %>%
  pivot_longer(c(-id, -year), names_to = "type", values_to = "wages")
```


```{r compare, echo = FALSE, fig.cap="Comparison between the original and the treated mean hourly wage. The orange line portray the original value of mean hourly wage, while the turquoise line display the mean hourly wages value after the extreme values imputed with the robust linear model's prediction value. We can see that some extreme spikes has been reduced by the model.", fig.height=5, fig.width=6}
ggplot(wages_compare) +
  geom_line(aes(x = year,
                y = wages,
                colour = type,
                linetype = type),
            alpha = 1) +
  geom_point(aes(x = year,
                y = wages,
                colour = type),
            alpha = 0.5,
            size = 1) +
  theme(axis.text.x = element_text(angle = 10, size = 5),
        legend.position = "bottom") +
  facet_wrap(~id, scales = "free_y")


```

Finally, we saved the imputed data and set the appropriate data type for the variables. We also saved the NLSY79 cohort's demographic information and the high school dropout cohort in a separate data sets. Here, we defined high school dropouts as respondents who only completed either $9^{th}$, $10^{th}$, $11^{th}$ grade, or who completed $12^{th}$ when their age are at least 19 years old (means that these IDs being dropped out in certain year, but they returned back to high school to complete it). We also filtered the data to be only male and aged between 14 and 17 years old to refresh the high school dropouts data in the `brolgar` package. 

We then make these three data sets and its processing documentation to be publicly available through an R data container package called `yowie`. The complete flow from the raw data to these data set is displayed in Figure \ref(fig:flowchart).

```{r}
# select out the old value of mean hourly wage and change it with the wages_rlm value
wages_demog_hs <- wages_demog_hs %>%
  dplyr::select(-mean_hourly_wage) %>%
  rename(mean_hourly_wage = wages_rlm)

# rename and select the wages in tidy
wages_hs2020 <- wages_demog_hs %>%
  dplyr::select(id, year, mean_hourly_wage, age_1979, gender, race, hgc, hgc_i, yr_hgc,
                number_of_jobs, total_hours, is_wm, is_pred) %>%
  mutate(hgc = as.factor(hgc),
         year = as.integer(year),
         age_1979 = as.integer(age_1979),
         yr_hgc = as.integer(yr_hgc),
         number_of_jobs = as.integer(number_of_jobs))

# Create a data set for demographic variables
demographic_nlsy79 <- full_demographics %>%
  mutate(age_1979 = 1979 - (dob_year + 1900)) %>%
  dplyr::select(id,
         age_1979,
         gender,
         race,
         hgc,
         hgc_i,
         yr_hgc) %>%
  mutate(age_1979 = as.integer(age_1979),
         hgc = as.factor(hgc),
         yr_hgc = as.integer(yr_hgc))

# Create a data set for the high school dropouts cohort
wages_hs_dropout <- wages_hs2020 %>%
  mutate(dob = 1979 - age_1979,
         age_hgc = yr_hgc - dob) %>%
  filter((hgc %in% c("9TH GRADE",
                     "10TH GRADE",
                     "11TH GRADE")) |
          (hgc == "12TH GRADE" &
              age_hgc > 19)) %>%
  filter(age_1979 <= 17,
         gender == "MALE") %>%
  dplyr::select(-dob,
         -age_hgc)

```




```{r nrow, echo=FALSE}
a = nrow(categories_qnames)
b = nrow(full_demographics)
d = eligible_wages %>%
  group_by(id) %>%
  count(id) %>%
  nrow()
n_hgc = wages_hs2020 %>%
  group_by(id) %>%
  count(id) %>%
  nrow()
n_obs_hgc = nrow(wages_hs2020)
n_obs_hgc_pred = filter(wages_hs2020, is_pred == TRUE) %>%
  nrow()

n_do = wages_hs_dropout %>%
  group_by(id) %>%
  count(id) %>%
  nrow()
n_obs_do = nrow(wages_hs_dropout)
n_obs_do_pred = filter(wages_hs_dropout, is_pred == TRUE) %>%
  nrow()
```


```{r flowchart, echo=FALSE, fig.cap="The stages of data filtering from the raw data to get three datasets that are contained in an R package called yowie, n means the number of ID, while n_obs means the number of observations.", fig.height=3, fig.width=3}

fc <- grViz("digraph flowchart {
      node [fontname = Helvetica, shape = rectangle]        
      tab1 [label = '@@1']
      tab2 [label = '@@2']
      tab3 [label = '@@3']
      tab4 [label = '@@4']
      tab5 [label = '@@5']
      tab6 [label = '@@6']
      tab7 [label = '@@7']

      tab1 -> tab2;
      tab1 -> tab3;
      tab1 -> tab4 -> tab5 -> tab6;
      tab5 -> tab7;
      tab6 -> tab7;
      tab2 -> tab7
      }

      [1]: paste0('NLSY79, n = ', a)
      [2]: paste0('Extract demographic variable, n = ', b)
      [3]: paste0('Exclude ', a - d, ' IDs whose hourly rate is missing')
      [4]: paste0('Eligible ID, n = ', d)
      [5]: paste0('Cohort whose hgc is up to 12th grade and \\n participated at least 5 years in the survey, \\n n = ', n_hgc, ', n_obs = ', n_obs_hgc, ' \\n (', n_obs_hgc_pred, ' observations are predicted value)')
      [6]: paste0('High school dropouts cohort, \\n n = ', n_do, ', n_obs = ', n_obs_do, ' \\n (', n_obs_do_pred, ' observations are predicted value)')
      [7]: 'yowie Package'
      ",
      height = 200)

fc
```



# Exploratory Data Analysis

In this part, we gave some examples of how the data might be used for data analysis in text book. We used `brolgar` [@brolgar] to perform exploratory data analysis. The first example is that we observed relationship between wages and education in terms of how the wages increase year by year. Using `key_slope` function in `brolgar`, we performed a liner regression with  mean hourly wage as response variable and year as predictor and extracted the regression slope of each ID and plotted it according to highest grade completed. 

We found in Figure \ref{fig:wages-slope} that the higher the education completed does not necessarily increase the wages more from year to year. We can see that people who completed 4th grade happen to have relatively the same median of slope as people who completed 11th grade, even though it is probably because some respondents in this group had an extremely high mean hourly wages. Moreover, the annual increasing wages of people who completed 6th grade to 11th grade are relatively same. People who completed 12th grade tend to have higher slope of year compared to the other groups. Some outliers are also spotted in this group. Additionally, we can see that some respondents have a negative slope, meaning that their wages tend to decrease over the year.

```{r wages-tsibble, echo = FALSE}
# create the wages data as tsibble
wages_hs2020_tsibble <- wages_hs2020 %>%
  as_tsibble(key = id,
             index = year,
             regular = FALSE)
```

```{r wages-slope, echo = FALSE, fig.cap="Regression slope of mean hourly wage regression by highest grade completed. The regression model has mean hourly wage as response variable and year as predictor. Respondents who completed 12th grade has the highest median of slope."}


wages_slope <- key_slope(wages_hs2020_tsibble, 
            mean_hourly_wage ~ year) %>%
  left_join(demographic_nlsy79, by = "id") %>%
  mutate(hgc = factor(hgc, levels = c("1ST GRADE", "2ND GRADE", "3RD GRADE", 
                                      "4TH GRADE", "5TH GRADE", "6TH GRADE", 
                                      "7TH GRADE", "8TH GRADE", "9TH GRADE",
                                      "10TH GRADE", "11TH GRADE", "12TH GRADE", 
                                      "UNGRADED"))) %>%
  filter(hgc != "UNGRADED")

ggplot(wages_slope, aes(y = hgc,
                      x = .slope_year)) +
  geom_boxplot()
```


In the second example, we examined the pattern of mean hourly wage corresponding to different groups of gender and race. We took 240 samples of female and male equally and spread it into twelve facets as is seen in Figure \ref{fig:gender-plot} using `facet_strata` function from `brolgar` [@brolgar]. We learn that the mean hourly wage of female and male fluctuated over the years. In some facets, for example in facet 4, 5, and 10, males tend to earn more wages than females. This finding is also shown in Figure \ref{fig:gender-plot2} where we plotted the median wage of males and females against the year. We learn that males had a higher mean of mean hourly wage over the period of survey than females. Unfortunately, the gap between them tend to be wider from time to time.


```{r gender-plot, echo = FALSE, warning = FALSE, fig.cap= "The pattern of mean hourly wages overtime displayed in 12 facets coloured by gender. The respondents belong to high school cohort are randomly sampled. We learn that both male and female respondents had a fluctuate mean hourly wage over the year." }
# sample the data by gender

set.seed(31251587)
wages_hs2020_male <- wages_hs2020_tsibble %>%
  filter(gender == "MALE") %>%
  sample_n_keys(size = 120)

set.seed(31251587)
wages_hs2020_female <- wages_hs2020_tsibble %>%
  filter(gender == "FEMALE") %>%
  sample_n_keys(size = 120)

wages_hs2020_sample_gender <- bind_rows(wages_hs2020_male, wages_hs2020_female)


ggplot(wages_hs2020_sample_gender, 
       aes(x = year,
           y = mean_hourly_wage,
           group = id,
           color = gender)) + 
  geom_line(alpha = 0.7) +
  scale_color_brewer(palette = "Dark2") +
  facet_strata(along = ~gender) 
```



```{r gender-plot2, echo=FALSE, fig.cap="The yearly mean of mean hourly wage by gender along with its standard error. Males generally earn more wages than females. The error interval tend to be wider over the years."}
wages_hs2020_grouped_gender <- wages_hs2020 %>%
  group_by(gender, year) %>%
  summarise(mean = mean(mean_hourly_wage),
             sd = sd(mean_hourly_wage),
             samplesize = n()) %>%
  ungroup() %>%
  mutate(upper = mean + sd/sqrt(samplesize),
         lower = mean - sd/sqrt(samplesize)) 

gender_plot <- ggplot(wages_hs2020_grouped_gender,
                      aes(x = year,
                          y = mean,
                          group = gender,
                          colour = gender)) +
  geom_point(size = 1) +
  geom_errorbar(aes(ymin=lower, ymax=upper), width=1,
                 position=position_dodge(0.1)) +
  theme(legend.position = "bottom",
         legend.text = element_text(size = 5),
         axis.title = element_text(size = 9)) +
  scale_color_brewer(palette = "Dark2") +
  ylab("median of mean hourly wage (US$)")

gender_plot

```

For the final example, we show the example of modeling the longitudinal data using Generelized Additive Model (GAM) with `mgcv` package [@mgcv] for each ID. We fit a more flexible model to fit in to the data since the relationship between mean hourly wage and year is not linear due to the spikes. Since this approach is computationally expensive, we only show the model in a little fraction of the data. We sample 1 percent of the total respondents using `brolgar's` `sample_frac_keys` function. Further, we plotted the some sample of the fitted model in Figure \@ref{fig:gam}. We learn that the model flexible enough to deal with the fluctuation in the data. Moreover, model with small spikes tend to be linearly fitted.  

```{r gam, cache=TRUE, echo=FALSE, fig.cap="Exploration of wages data by fitting a GAM. The fitted model displayed by blue line. The fitted line shows that the model flexible enough to follow the pattern of the data."}

# take a small fraction of key
set.seed(2020)
wages_sample_gam <- sample_frac_keys(wages_hs2020_tsibble, size = 0.01) %>%
  mutate(id_fct = as.factor(id),
         year0 = year - 1979)

# build GAM model
wages_gam <- gam(
  mean_hourly_wage ~ s(year0, by = id_fct) + id_fct,
  data = wages_sample_gam,
  method = "REML"
)

wages_aug_gam <- wages_sample_gam %>%
  add_predictions(wages_gam, var = "pred") %>%
  add_residuals(wages_gam, var = "res") %>%
  group_by_key() %>%
  mutate(rss = sum(res^2)) %>%
  ungroup()

set.seed(2021)
wages_aug_gam %>%
  sample_n_keys(12) %>%
  ggplot(aes(x = year,
             y = pred,
             group = id)) +
  geom_line(color = "steelblue") +
  geom_point(aes(y = mean_hourly_wage)) +
  facet_wrap(~id)

```

# Summary

This paper has performed a set of stages to make an open data suitable for a text book data or to make it ready for research. In the first stage, we showed the steps that are performed to get the data from the NLSY79 database. Since the data format is untidy, we showed how the data has been tidied. After that, we conducted an initial data analysis to investigate and screen the quality of the data. Using robust linear regression model, we found and fixed the anomalous observations in the data set with its predicted values. We also performed an example of exploratory data analysis using the cleaned data set. 

This paper also has demonstrated the documentation of data cleaning by providing all of the codes in performing data tidying and initial data analysis. Thus, this paper provided an opportunity to continuously refresh the text book data whenever the updated data is published in the NLSY79 database. This could be done by following the documentation of the code that is provided in this paper. 

Moreover, the documentation also includes the process of how did we generate the robustness weight and how did we decide the threshold of anomalous observations. It is also documented along with the flag of whether an observation is imputed value or not. Accordingly, if somebody wish to make another decision, it can be done by making a small changes in the code provided. Further, this paper is also supplemented by a `shiny`  [@shiny] app as a simulation tool to customize the weight threshold. 

Finally, this paper implies that data providers should design the database that is able to produce tidy data sets. A data provider should also check for data anomalies prior the data publishing or at least provides a set of rules or threshold value, for example, in this case is the threshold of reasonable wages. This will greatly support the data users to carry out further validation and set the same understanding of which data are considered as outliers. Moreover, providing validation rules would facilitate the usage of any establish data validation tool, such as `validate` [@validate] package. In this case, we cannot use this handy validation package due to the absence of validation rules.  


# Acknowledgements 

We would like to thank Aarathy Babu for the insight and discussion during the writing of this paper. 

The entire analysis is conducted using `R` [@R] in `rstudio` using these packages: `tidyverse` [@tidyverse], `ggplot2` [@ggplot2], `dplyr` [@dplyr], `readr` [@readr], `tidyr` [@tidyr], `stringr` [@stringr], `purrr` [@purrr], `broom` [@broom], `blorgar` [@brolgar], `patchwork` [@patchwork], `kableExtra` [@kableExtra], `MASS` [@mass], `janitor` [@janitor], `DiagrammeR` [@DiagrammeR], `rsvg` [@rsvg], `webshot` [@webshot], `mgcv` [@mgcv], `tsibble` [@tsibble], and `modelr` [@modelr]. The paper are generated using `knitr` [@knitr], `rmarkdown` [@rmarkdown], and `rticles` [@rticles]. 


# Supplementary Materials 


**Codes** : R script to reproduce data tidying and cleaning are available in this [Github Repository](https://github.com/numbats/yowie/blob/master/data-raw/data_preprocessing.R)

**R Package `yowie`**:`yowie` is a data container R package that contains 3 datasets, namely the high school mean hourly wage data, high school dropouts mean hourly wage data, and demographic data of the NLSY79 cohort. This package could be accessed [here](https://github.com/numbats/yowie).

**shiny app**: An web interactive `shiny` app to run a simulation to customize the weight threshold. This app could be accessed [here](https://github.com/numbats/summer-wages-refresh/tree/main/app).



# References
